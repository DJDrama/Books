[14 Security]
	- CIA (confidentiality, integrity, and availability)
	- MFA (Multi-factor authentication)
	- GPDR (General Data protection regulation)
	- HIPAA (Health insurance portability and accountability act)

(The importance of security in software architecture)
	- "System security"
		-> processes, measures, and practices implemented to protect information systems from unauthorized access, misuse, damage, or disruption.
		-> includes a wide range of mechanisms aimed at safeguarding the confidentiality, integirty, and availability of data and resources within a system.

	- handle sensitive information.
		1) Personally Identifiable Information (PII)
			-> full names, dates of birth, addresses, telephone numbers, and personal email addresses.

		2) Personal information
			-> Health insurance policies, medical test results, treatment records, prescriptions, education certificates, transcripts, university grades, and student identification numbers

		3) Authentication credentials
			-> Passwords, PINs, security questions and answers, and fingerprints

		4) Financial Information
			-> Bank account details, credit card numbers, tax returns, financial statements, and income information

		5) Confidential business information
			-> Client lists, business plans, tade secrets, and internal communications

		6) Legal documents
			-> Contracts, litigation documents, and settlement agreements

		7) Intellectual property
			-> Copyrights, patents, trademarks, source code, user activity history, system data, and proprietary algorithms

		8) Government and national security information
			-> Government contracts, intelligence reports, and classified documents

(The CIA triad)
	- a foundational model in information security that outlines the three core principles of information security.
		+ Confidentiality
		+ Integrity
		+ Availability

	- initially mentioned in "Computer Security Technology Planning Study", known as "The Anderson Report", in 1972, authored by a team led by William Anderson.

	<Confidentiality>
		- ensures that sensitive information is only accessible to authorized individuals or systems.

		- common methods to maintain confidentiality include the following:
			1) Authentication
				-> Confirm the identity of a user, device, or system.
				-> goal is to ensure that an entity trying to access a system is who they claim to be.

			2) Authorization
				-> grant or deny an authenticated identity to specific resources, actions, or data within a system.

			3) Encryption
				-> convert readable data into encoded data to prevent unauthorized access.

			4) Redaction
				-> retain functionality and usuability without exposing sensitive information.

	<Integrity>
		- accuracy and consistency of data throughout its life cycle.
		- ensures that information is not altered or tampered with by unauthorized users and that it remains accurate, trustworthy, and reliable.

		- common techniques to ensure integrity:
			1) Checksums and hash functions
				-> a fixed-length string calculated from the data by an algorithm.
				-> any tampering with the data would produce a checksum different from the original checksum, so it is detected by the systems as corrupted data.

			2) Digital signatures
				-> a document is signed by a hash value generated by a private key from the sender.
				-> the recipient receives the document together with the digital signature.
				-> the recipient decrypts the digital signature using the sender's public key and retrieves the original hash value.
				-> two hash values being identical confirms that the document has not been modified.
				-> the sender's private and public keys from a pair, whereas the private key is only known to the sender and the public key is available to anyone.

			3) Versioning control
				-> maintain a complete audit trail and history of changes made to a document.
				-> if errors or corruptions are detected, the document can be reverted a previous version.

	<Availability>
		- ensures that information and resources are accessible to authorized users when needed.
		- focuses on maintaining system functionality and minimizing downtime due to attacks, failures, or other disruptions.

		- strategies to enhance availability include the following:
			1) Redundancy
				-> avoid a single point of failure by having extra components and alternative paths to ensure continued operations and data integrity in the event of failure.

			2) Load balancing
				-> distribute information traffic across multiple servers to ensure incoming requests are served.

			3) Regular backups
				-> maintain copies of the database, file storage and messaging store across multiple servers or locations to ensure data availability and to recover from data issues.

			4) Disaster recoery planning and drills
				-> outline specific steps to take during a disaster to bring the system back up and make it operational.

	<Importance of the CIA triad>
		- CIA triad serves as a guiding framework for organizations to develop and implement effective security policies and practices.

		- three principles help organizations create a comprehensive approach to information security and protect the business from various threats and vulnerabilities.

(Authentication)
	- process of verifying the identity of a user or a device before granting acccess to resources.

	- "MitM attck" (Man in the Middle attack)
		-> attacker intercepts and relays communication between two parties.
		-> attacker eavesdrops and captures communication potentially containing sensitive information.
		-> attacker might alter the messages exchanged or record them for further malicious intents, such as identity theft and financial fraud.
			ex)
				+ intercepting unencrypted Wi-Fi traffic
				+ exploiting vulnerabilities in secure communications
				+ tricking users into connecting to malicious networks

	<Transport Layer Security (TLS)>
		- evolved from its predecessor, "Secure Socket Layer" (SSL)
		- first released in 1999. based on SSL 3.0 addressing the weaknesses of SSL and provided better support for modern cryptographic algorithms.

		- built on top of "Transmission Control Protocol" (TCP).

		- steps:
			1) messages exchanged to estabilsh a TCP connection.
			2) few exchanges of messages to establish a TLS communication:
				[1] Client hello
					-> initiates TLS by sending to the server a message specifying supported TLS versions, cipher suites (known as encryption algorithms), and a random number generated by the client.

				[2] Server hello
					-> responds with its chosen TLS version and cipher suite, and a random number generated by the server

				[3] Server certificate
					-> server sends its TLS certificate containing the server's public key and is signed by a trusted ceritifcate authority (CA).

				[4] Server hello done
					-> server aknowledges finishing its part of the handshake

				[5] Client key exchange
					-> client generates a pre-master secret, which is a random string without meaningful data, encrypts it with the server's public key received from the server certificate, and sends it to the server.

				[6] Session keys creation
					-> both the client and server use the pre-master secret along with the previously exchanged random numbers to generate session keys.
					-> these keys will be used for encrypting the data during the session.

				[7] Finished messages
					-> client and server exchange "Finished" messages, indicating that the handshake is complete and that they will now start using the session keys for secure communication.

		- each message contains "Message Authentication Code"(MAC) that acts like a checksum to confirm whether the data has been altered in transit.

		- TLS uses digital ceritificates issued by trusted CAs to authenticate the server's identity.
			-> prevents MitM attacks, where an attacker impersonates the server.

	<Multi-factor authentication (MFA)>
		- simple authentication requires only one such piece of evidence, typically a password.
			-> has several weaknesses that make it inadequate for securing sensitive information:
				1) Weak passwords
					-> easily guessed by attackers

				2) Phishing
					-> attackers trick users into revealing their passwords (fake landing pages)

				3) Password reuse
					-> if one password is compromised, attackers can access other services using the same password

				4) Brute force
					-> automated tools can keep guessing the one password

				5) Social engineering
					-> manipulate users into sharing passwords

				6) No user verification
					-> password verification only checks whether the provided credentials are the same as the ones in the record, instead of verifying that the person attempting to gain access is the actual user

		- MFA aims to harden the authentication process by verifying multiple factors, further validating that the user is who they claim to be.
			1) Knowledge factor
				-> passwords, security questions and answers, PINs, or IDs

			2) Possession factor
				-> Smartphones, hardware tokens, authentication applications, and one-time passcodes

			3) Biometric factor
				-> fingerprints, faces, and voices

		- MFA requires at least two factors from distinct categories.
		- Some industries, such as banking, have regulatory requirements to enforce the use of MFA to protect sensitive data.

	<Implications of MFA to software architecture>
		- every organization that runs its services in the cloud must choose one of the following:
			1) "Identity providers" (IdPs) native to the cloud providers
			2) Platform independent IdPs, such as Okta, Auth0, and Duo Security
			3) Write its own "Identity and Access Management" (IAM) service, optionally as a proxy to other IdPs
			4) No authentication required

		- "One-time passcode" (OTP)
			-> usually sent to the user's phone number via "Short Message Service" (SMS)
			-> IdP verifies whether the OTP is correct or not.
			-> if it's incorrect, the Idp responds to the client and revelas that the authentication ha failed.
			-> if it's correct, the IdP generatees "JSON Web Token"(JWT) as an access token and sends it to the client as a response.

		- "JSON Web Token" (JWT)
			1) Header
				-> the type of the token as "JWT" and the algorithm used to encrypt the token

			2) Payload
				-> Registered claims, such as issuer, audience, expiration time, and ID.
				-> Also contains public claims, such as application user roles, and private claims for information to be shared only between parties using the token.

			3) Signature
				-> The proof that the sender is who it claims to be and the whole JSON string has not been modified.

		- "OAuth"
			-> an authorization framework that enables third-party applications to obtain limited access to resources without exposing credentials.

		- "Step-up authentication"
			-> sometimes the authenticated user is asked to perform additional verification beyond their initial authentication.
				
				ex) when they involve accessing sensitive information or high-risk actions
					+ updating passwords
					+ making a bank transfer
					+ moving the account data to an unfamiliar device

			-> can be dynamically applied based on the context of the access requests, such as a new geographic location, new device, new bank account, and so on.

			-> appraoch is to repeat the same MFA process under TLS communication.

(Authorization)
	- determines what resources a user or entity is allowed to access and what actions they can perform.

	- restricts sensitive information to only individuals the system knows and maintains the integrity of systems.

	- conforms to the "principle of least privilege" (PoLP), known as the "principle of minimal privilege" (PoMP) or the "principle of least authority" (PoLA).

	- "Principle of least privilege" (PoLP)
		-> states that a user, entity, or system should have only the essential permissions to operate its functions.
		-> minimizes risks and limits potential damage from accidents or malicious actions that we might not have anticipated or known.

	- authorization has two elements: data and actions.
		1) Data entitlement: whther the user is entitled to access specific data or resources.

		2) PErmission: which actions the user is allowed to perform on a resource or data

	<Role-based access control (RBAC)>
		- each role has specific permissions assigned to it.
		- more efficient than assigning permissions to users individually.

	<Access Control Lists (ACLs)>
		- grants a list of users or groups of users to have access to a specific resource and defines which actions are allowed.
			-> suitable for granting permissions to resources that can be precisely specified.
				1) Filesystems: Folders, read / write actions, and filenamees
				2) HTTP paths: Uniform Resource Identifiers (URIs) and Http methods
				3) Networking: IP addresses, ports and transport layer protocols
				4) Database: Schemas, tables, and actions

	<Policy-based access control (PBAC)>
		- grants permissions based on pre-defined policies that consider various criteria, such as user roles, attributes, and context.

		- useful in dynamic environments where access needs to adapt based on real-time conditions or compliance requirements.

		- effective for compliance regulatory requirements because well-defined policies can be treated as a technical implementation of compliance policies.

	<Attribute-based access control (ABAC)>
		- grants permissions to attributes associated with users, resources, and the environment.

		- provides the most flexible way to control permissions that can be sensitive to the context such as time and geographic locations.

(Handling sensitive data)
	<Data classification>
		- three categories of data sensitivity:
			1) Confidential
				-> Only designated persons or roles can access

			2) Internal use
				-> Only members of the organization can access

			3) Public
				-> Accessible to all

	<Data in transit>
		- Data in transit under TLS communication is encrypted with session keys.
			-> ensures that even if data is intercepted or accessed without authorization, it remains unreadable.

		[How to prevent accidentally logging sensitive information?]
			- information leaked by logging unknowingly:
				1) Full decrypted payload of requests, repsonses, or messages
				2) An aggregate Kotlin object that transitively contains a sensitive field
				3) An object of a generated class that contains a sensitive field
				4) Troubleshooting or debugging information that contains sensitive information
				5) Sensitive field themselves

			- A common technique adopted by engineers is to override the "toString" function of a Kotlin data class.
				ex) BUT not scalable! (because there are a lot of override functions to write!)
					=============================================================
					data class UserAccount(
						val username: String,
						val password: String,
						val createdAt: Instant
					) {
						override fun toString(): String {
							return "UserAccount(createdAt=$createdAt)"
						}
					}
					=============================================================

			- Use "Redacted" library (https://github.com/ZacSweers/redacted-compiler-plugin)
				ex)
					=============================================================
					data class UserAccount(
						val username: String,
						@Redacted val password: String, // will be hidden
						@Redacted val createdAt: Instant // will be hidden
					)

					>> UserAccount(username=Bob, password=*, createdAt=*)
					=============================================================

	<Data at rest>
		- for sensitive information, additional steps need to be taken

		[Encryption]
			- common techniques:
				1) Encrypted databases
					-> all data stored in a database is stored in an encrypted and unreadable format.
					-> usually support authorization to ensure that only authorized users can interact with encrypted data.

					* However, encrypting / decrypting increases latency.

				2) Encrypted fields
					-> only fields identified as sensitive data are encrypted.
					-> less performance overhead compared to encrypted databases.

					* However, doing it at a field level implies that applications need to handle aspects that are automatically by encrypte databases, such as the following:
						+ Encryption / decryption algorithms
						+ Symmetric or asymmetric encryption key
						+ Key generation and secret management
						+ Key rotation and re-encryption

				3) Hybrid
					-> only sensitive data are stored in encrypted databases, and the rest are kept in any type of storage.

				4) Encrypted backups
					-> sensitive data should be regularly backed up in encrypted format to ensure data can be restored security in case of data loss or breaches.

	<Data retention and anonymization>
		- Local authorities and regulations have clear guidelines on how long data should be maintained, including sensitive data.
			-> During the retention period, it is the organization's responsibility to keep them secure and safe.
			-> After the retention period, sensitive data may be deleted.

		- "GDPR"
			-> individuals have the right to request for privacy data to be eraased from the system.

		- several techniques that allow sensitive data to be anonymized and become not sensitive:
			1) Masking and substitution
				-> replace real sensitive fields, such as email addresses and bank accounts, with known mask values (eg., a real email address is anonymized as anonymized@data.com)

			2) Generalization
				-> reduce the precision of sensitive values so they cannot identify a specific person or record (e.g., dates of birth reduced to months of birth, addresses trimmed to cities, etc.)

			3) Aggregation
				-> summarize data into statistics so there is no reference to a person or a record.

			4) Lost decryption keys
				-> delete the decryption keys so the data cannot be traced back to the individuals.

			5) Periodic anonymization
				-> aggressively scan data that has passed the retention period and anonymize it.

			6) Broken link
				-> An upfront database schema design that separates sensitive data and non-sensitive into different tables, and no tables have reference to the sensitive data table.
				-> sensitive data record can be deleted anytime without issues.

(Network Security)
	<Web application firewalls(WAFs)>
		- a security solution specialized in protecting systems on the internet
		- provides a few key functions:
			1) Geo-blocking and IP blacklisting
				-> block traffic coming from and going to the list of IP addresses linked to malicious activity and block traffic from certain geographic regions.

			2) Rate limiting
				-> prevent the number of requests to the server from reaching the threshold in duration as per configuration.

			3) Prevention of foreign script executions
				-> prevent attackers from executing scripts that are not part of the system, such as unauthorized database commands, "cross-site scripting(XSS)" from seb pages, and trojans.

			4) Policy-based or rule-based access policies
				-> highly configurable and customizable rules to set up the needs of each application at a fine-grained level.

			5) Frequent updates
				-> WAFs are updated frequently to adapt to the ever-changing landscape of security threats.

	<Traffic routing and network segmentation>
		- explicitly configured traffic routes allow applying features such as the following:
			1) Distributed tracing
				-> link requests that pass through multiple components as a holistic flow.

			2) Metrics
				-> collect metrics such as packet loss, response time, connection time, and error rate.

			3) Retrying
				-> allow applications to retry an operation in the face of intermittent failures.

			4) Circuit breaking
				-> disallow applications to repeat operations that have been failing.

			5) Rate limiting
				-> overlapped with the feautre of WAF, limit the number of requests over a period.

		- "Sidecar pattern"
			-> an architectural design approach where a secondary service, known as a sidecar, is deployed alongside a primary service to extend its functions but to decouple from its code base.

			-> this sidecar runs in the same container of VM.
			-> handles cross-cutting concerns, such as logging, monitoring, security, and communication.

			* sidecar handles auxiliary tasks (logging, monitoring, networking, or security) so the main application focues on its core logic.

			* enhances the resilience and maintainability of services, but without coupling with them, and thus facilitates more efficient management of complex applications.

	<Antivirus and anti-malware solutions>
		- monitor running processes, scan stored files, and discover suspicious behaviors without manual intervention.

(DevSecOps)
	- embraces the following principles:
		1) Shift left
			-> security considerations are integrated early in the development process, as a part of the requirements.
			-> enables the discovery, identification, and remediation of vulnerabilities before the application reaches production.

		2) Compliance as code
			-> include regulatory requirements in the code with automated tests to validate compliance continuously.

		3) Collaboration and communication
			-> encourages collaboration between development, security, and operations teams, fostering a culture of shared responsibility for software security.

		4) Automation
			-> automate security processes, such as code vulnerability scanning, as a part of "continuous integration / continuous deployment(CI/CD)"

		5) Continuous monitoring
			-> continously monitor security threats, vulnerabilities, and unusual behaviors in applications and infrastructure while running in all environments.

		6) Threat modeling
			-> discover and identify potential threats and vulnerabilities during the early stage of development.

	<Benefits of DevSecOps>
		- reduces the risk of security breaches, the cost of fixing security issues, and the cost of remediation.

		- Automation integrated within a CI/CD pipeline and collaboration helps find vulnerabilities early in the process.
			-> saves time indelivering secure applications and reduces the time to market.

		- continous monitoring and automated compliance checks help organizations meet regulatory requirements proactively and more effectively.


		
